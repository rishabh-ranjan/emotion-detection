{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch import tensor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.metrics import r2_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDataset(Dataset):\n",
    "    def __init__(self, index_file):\n",
    "        self.df = pd.read_csv(open(index_file))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        Y = tensor(self.df.iloc[i][0])\n",
    "        X = tensor(self.df.iloc[i][1:])\n",
    "        return (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = FaceDataset('/home/cse/dual/cs5180404/col774/train.csv')\n",
    "batch_size = 32\n",
    "train_data_loader = DataLoader(train_data, batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(2304, 100)\n",
    "        self.fc2 = nn.Linear(100, 7)\n",
    "        self.output = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 2304)\n",
    "        f1 = F.relu(self.fc1(x))\n",
    "        f2 = F.relu(self.fc2(f1))\n",
    "        return self.output(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "NNet(\n",
      "  (fc1): Linear(in_features=2304, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=7, bias=True)\n",
      "  (output): Softmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "net = NNet()\n",
    "net = net.to(device)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-4)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 1, mini-batch 20, loss: 38.88921129703522\n",
      "After epoch 1, mini-batch 40, loss: 38.5589919090271\n",
      "After epoch 1, mini-batch 60, loss: 38.37402963638306\n",
      "After epoch 1, mini-batch 80, loss: 37.569982051849365\n",
      "After epoch 1, mini-batch 100, loss: 37.74152374267578\n",
      "After epoch 1, mini-batch 120, loss: 37.39133965969086\n",
      "After epoch 1, mini-batch 140, loss: 37.618449687957764\n",
      "After epoch 1, mini-batch 160, loss: 37.07066833972931\n",
      "After epoch 1, mini-batch 180, loss: 37.17407929897308\n",
      "After epoch 1, mini-batch 200, loss: 37.277369260787964\n",
      "After epoch 1, mini-batch 220, loss: 36.87316644191742\n",
      "After epoch 1, mini-batch 240, loss: 36.95002233982086\n",
      "After epoch 1, mini-batch 260, loss: 37.07855558395386\n",
      "After epoch 1, mini-batch 280, loss: 36.98200750350952\n",
      "After epoch 1, mini-batch 300, loss: 36.686439514160156\n",
      "After epoch 1, mini-batch 320, loss: 36.987884759902954\n",
      "After epoch 1, mini-batch 340, loss: 36.60460722446442\n",
      "After epoch 1, mini-batch 360, loss: 36.73640716075897\n",
      "After epoch 1, mini-batch 380, loss: 36.831581592559814\n",
      "After epoch 1, mini-batch 400, loss: 37.08340871334076\n",
      "After epoch 1, mini-batch 420, loss: 36.47069001197815\n",
      "After epoch 1, mini-batch 440, loss: 36.30877506732941\n",
      "After epoch 1, mini-batch 460, loss: 36.4511616230011\n",
      "After epoch 1, mini-batch 480, loss: 36.31785202026367\n",
      "After epoch 1, mini-batch 500, loss: 35.91276955604553\n",
      "After epoch 1, mini-batch 520, loss: 36.58890390396118\n",
      "After epoch 1, mini-batch 540, loss: 36.402971148490906\n",
      "After epoch 1, mini-batch 560, loss: 36.57184815406799\n",
      "After epoch 1, mini-batch 580, loss: 36.90848672389984\n",
      "After epoch 1, mini-batch 600, loss: 36.126209020614624\n",
      "After epoch 2, mini-batch 20, loss: 35.99232876300812\n",
      "After epoch 2, mini-batch 40, loss: 35.86101162433624\n",
      "After epoch 2, mini-batch 60, loss: 35.32385039329529\n",
      "After epoch 2, mini-batch 80, loss: 35.72525978088379\n",
      "After epoch 2, mini-batch 100, loss: 36.389625906944275\n",
      "After epoch 2, mini-batch 120, loss: 35.42762041091919\n",
      "After epoch 2, mini-batch 140, loss: 36.314282178878784\n",
      "After epoch 2, mini-batch 160, loss: 36.16263544559479\n",
      "After epoch 2, mini-batch 180, loss: 36.44581937789917\n",
      "After epoch 2, mini-batch 200, loss: 36.42939281463623\n",
      "After epoch 2, mini-batch 220, loss: 35.71768283843994\n",
      "After epoch 2, mini-batch 240, loss: 35.887476086616516\n",
      "After epoch 2, mini-batch 260, loss: 35.547606110572815\n",
      "After epoch 2, mini-batch 280, loss: 35.70196771621704\n",
      "After epoch 2, mini-batch 300, loss: 35.77139365673065\n",
      "After epoch 2, mini-batch 320, loss: 36.20507788658142\n",
      "After epoch 2, mini-batch 340, loss: 35.86543381214142\n",
      "After epoch 2, mini-batch 360, loss: 35.74405062198639\n",
      "After epoch 2, mini-batch 380, loss: 35.944992661476135\n",
      "After epoch 2, mini-batch 400, loss: 35.685293555259705\n",
      "After epoch 2, mini-batch 420, loss: 35.592408657073975\n",
      "After epoch 2, mini-batch 440, loss: 35.70423936843872\n",
      "After epoch 2, mini-batch 460, loss: 35.6352744102478\n",
      "After epoch 2, mini-batch 480, loss: 36.171958208084106\n",
      "After epoch 2, mini-batch 500, loss: 35.614835262298584\n",
      "After epoch 2, mini-batch 520, loss: 36.06623029708862\n",
      "After epoch 2, mini-batch 540, loss: 36.08491933345795\n",
      "After epoch 2, mini-batch 560, loss: 35.29799485206604\n",
      "After epoch 2, mini-batch 580, loss: 36.32520866394043\n",
      "After epoch 2, mini-batch 600, loss: 35.99465000629425\n",
      "After epoch 3, mini-batch 20, loss: 35.575446367263794\n",
      "After epoch 3, mini-batch 40, loss: 34.87508773803711\n",
      "After epoch 3, mini-batch 60, loss: 35.32607567310333\n",
      "After epoch 3, mini-batch 80, loss: 35.3274781703949\n",
      "After epoch 3, mini-batch 100, loss: 35.39292371273041\n",
      "After epoch 3, mini-batch 120, loss: 35.554474234580994\n",
      "After epoch 3, mini-batch 140, loss: 35.99587523937225\n",
      "After epoch 3, mini-batch 160, loss: 35.615503907203674\n",
      "After epoch 3, mini-batch 180, loss: 35.649123311042786\n",
      "After epoch 3, mini-batch 200, loss: 35.16461133956909\n",
      "After epoch 3, mini-batch 220, loss: 35.23139142990112\n",
      "After epoch 3, mini-batch 240, loss: 35.458052277565\n",
      "After epoch 3, mini-batch 260, loss: 35.62193560600281\n",
      "After epoch 3, mini-batch 280, loss: 35.62829005718231\n",
      "After epoch 3, mini-batch 300, loss: 35.44622230529785\n",
      "After epoch 3, mini-batch 320, loss: 35.728830456733704\n",
      "After epoch 3, mini-batch 340, loss: 35.496867656707764\n",
      "After epoch 3, mini-batch 360, loss: 35.71725130081177\n",
      "After epoch 3, mini-batch 380, loss: 35.28383648395538\n",
      "After epoch 3, mini-batch 400, loss: 34.83540368080139\n",
      "After epoch 3, mini-batch 420, loss: 35.63873291015625\n",
      "After epoch 3, mini-batch 440, loss: 35.31571567058563\n",
      "After epoch 3, mini-batch 460, loss: 35.57196390628815\n",
      "After epoch 3, mini-batch 480, loss: 35.61311650276184\n",
      "After epoch 3, mini-batch 500, loss: 36.036033153533936\n",
      "After epoch 3, mini-batch 520, loss: 35.116228103637695\n",
      "After epoch 3, mini-batch 540, loss: 35.85391581058502\n",
      "After epoch 3, mini-batch 560, loss: 35.4378559589386\n",
      "After epoch 3, mini-batch 580, loss: 35.934027671813965\n",
      "After epoch 3, mini-batch 600, loss: 35.21686816215515\n",
      "After epoch 4, mini-batch 20, loss: 35.53561520576477\n",
      "After epoch 4, mini-batch 40, loss: 34.75288259983063\n",
      "After epoch 4, mini-batch 60, loss: 35.376797914505005\n",
      "After epoch 4, mini-batch 80, loss: 35.10271203517914\n",
      "After epoch 4, mini-batch 100, loss: 35.40065395832062\n",
      "After epoch 4, mini-batch 120, loss: 35.21456503868103\n",
      "After epoch 4, mini-batch 140, loss: 35.12197661399841\n",
      "After epoch 4, mini-batch 160, loss: 35.50446176528931\n",
      "After epoch 4, mini-batch 180, loss: 34.7508088350296\n",
      "After epoch 4, mini-batch 200, loss: 34.623138427734375\n",
      "After epoch 4, mini-batch 220, loss: 35.0672721862793\n",
      "After epoch 4, mini-batch 240, loss: 35.16727960109711\n",
      "After epoch 4, mini-batch 260, loss: 34.93937563896179\n",
      "After epoch 4, mini-batch 280, loss: 35.824209570884705\n",
      "After epoch 4, mini-batch 300, loss: 35.51511251926422\n",
      "After epoch 4, mini-batch 320, loss: 35.21628654003143\n",
      "After epoch 4, mini-batch 340, loss: 35.39229607582092\n",
      "After epoch 4, mini-batch 360, loss: 35.410581946372986\n",
      "After epoch 4, mini-batch 380, loss: 35.417914390563965\n",
      "After epoch 4, mini-batch 400, loss: 35.238964319229126\n",
      "After epoch 4, mini-batch 420, loss: 35.06407833099365\n",
      "After epoch 4, mini-batch 440, loss: 35.201701402664185\n",
      "After epoch 4, mini-batch 460, loss: 35.15596902370453\n",
      "After epoch 4, mini-batch 480, loss: 35.053643107414246\n",
      "After epoch 4, mini-batch 500, loss: 35.252241015434265\n",
      "After epoch 4, mini-batch 520, loss: 34.55208706855774\n",
      "After epoch 4, mini-batch 540, loss: 35.03725278377533\n",
      "After epoch 4, mini-batch 560, loss: 35.012097120285034\n",
      "After epoch 4, mini-batch 580, loss: 35.61676335334778\n",
      "After epoch 4, mini-batch 600, loss: 35.50032615661621\n",
      "After epoch 5, mini-batch 20, loss: 35.595550298690796\n",
      "After epoch 5, mini-batch 40, loss: 34.73927569389343\n",
      "After epoch 5, mini-batch 60, loss: 34.59614098072052\n",
      "After epoch 5, mini-batch 80, loss: 34.85103225708008\n",
      "After epoch 5, mini-batch 100, loss: 34.502135157585144\n",
      "After epoch 5, mini-batch 120, loss: 34.82190144062042\n",
      "After epoch 5, mini-batch 140, loss: 35.170934319496155\n",
      "After epoch 5, mini-batch 160, loss: 34.53145182132721\n",
      "After epoch 5, mini-batch 180, loss: 34.81392800807953\n",
      "After epoch 5, mini-batch 200, loss: 34.91156506538391\n",
      "After epoch 5, mini-batch 220, loss: 34.65610647201538\n",
      "After epoch 5, mini-batch 240, loss: 34.758469462394714\n",
      "After epoch 5, mini-batch 260, loss: 34.69225895404816\n",
      "After epoch 5, mini-batch 280, loss: 35.30111312866211\n",
      "After epoch 5, mini-batch 300, loss: 35.83060383796692\n",
      "After epoch 5, mini-batch 320, loss: 35.32456612586975\n",
      "After epoch 5, mini-batch 340, loss: 34.539756655693054\n",
      "After epoch 5, mini-batch 360, loss: 35.030234813690186\n",
      "After epoch 5, mini-batch 380, loss: 34.988860726356506\n",
      "After epoch 5, mini-batch 400, loss: 34.79666829109192\n",
      "After epoch 5, mini-batch 420, loss: 34.68816089630127\n",
      "After epoch 5, mini-batch 440, loss: 35.008658051490784\n",
      "After epoch 5, mini-batch 460, loss: 35.35370373725891\n",
      "After epoch 5, mini-batch 480, loss: 34.82667326927185\n",
      "After epoch 5, mini-batch 500, loss: 34.717832922935486\n",
      "After epoch 5, mini-batch 520, loss: 35.50729763507843\n",
      "After epoch 5, mini-batch 540, loss: 35.65315222740173\n",
      "After epoch 5, mini-batch 560, loss: 34.83038127422333\n",
      "After epoch 5, mini-batch 580, loss: 35.066359758377075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 5, mini-batch 600, loss: 35.48583102226257\n",
      "After epoch 6, mini-batch 20, loss: 34.415576100349426\n",
      "After epoch 6, mini-batch 40, loss: 34.540515422821045\n",
      "After epoch 6, mini-batch 60, loss: 34.48539972305298\n",
      "After epoch 6, mini-batch 80, loss: 34.61926865577698\n",
      "After epoch 6, mini-batch 100, loss: 34.95616018772125\n",
      "After epoch 6, mini-batch 120, loss: 34.597689628601074\n",
      "After epoch 6, mini-batch 140, loss: 35.46968472003937\n",
      "After epoch 6, mini-batch 160, loss: 34.529619455337524\n",
      "After epoch 6, mini-batch 180, loss: 35.36648452281952\n",
      "After epoch 6, mini-batch 200, loss: 34.32918167114258\n",
      "After epoch 6, mini-batch 220, loss: 35.14157032966614\n",
      "After epoch 6, mini-batch 240, loss: 34.77009344100952\n",
      "After epoch 6, mini-batch 260, loss: 34.483880400657654\n",
      "After epoch 6, mini-batch 280, loss: 35.180511474609375\n",
      "After epoch 6, mini-batch 300, loss: 35.025996804237366\n",
      "After epoch 6, mini-batch 320, loss: 34.78375005722046\n",
      "After epoch 6, mini-batch 340, loss: 34.34204924106598\n",
      "After epoch 6, mini-batch 360, loss: 34.579163789749146\n",
      "After epoch 6, mini-batch 380, loss: 34.51292276382446\n",
      "After epoch 6, mini-batch 400, loss: 34.99731516838074\n",
      "After epoch 6, mini-batch 420, loss: 35.27751922607422\n",
      "After epoch 6, mini-batch 440, loss: 35.00645387172699\n",
      "After epoch 6, mini-batch 460, loss: 34.724485754966736\n",
      "After epoch 6, mini-batch 480, loss: 35.249552607536316\n",
      "After epoch 6, mini-batch 500, loss: 34.53379762172699\n",
      "After epoch 6, mini-batch 520, loss: 34.90460741519928\n",
      "After epoch 6, mini-batch 540, loss: 34.942490220069885\n",
      "After epoch 6, mini-batch 560, loss: 34.40425479412079\n",
      "After epoch 6, mini-batch 580, loss: 34.93038499355316\n",
      "After epoch 6, mini-batch 600, loss: 34.84909248352051\n",
      "After epoch 7, mini-batch 20, loss: 35.1496559381485\n",
      "After epoch 7, mini-batch 40, loss: 34.39267909526825\n",
      "After epoch 7, mini-batch 60, loss: 34.309465408325195\n",
      "After epoch 7, mini-batch 80, loss: 34.20415687561035\n",
      "After epoch 7, mini-batch 100, loss: 34.691547989845276\n",
      "After epoch 7, mini-batch 120, loss: 34.74942374229431\n",
      "After epoch 7, mini-batch 140, loss: 34.74187481403351\n",
      "After epoch 7, mini-batch 160, loss: 34.7850022315979\n",
      "After epoch 7, mini-batch 180, loss: 34.39574384689331\n",
      "After epoch 7, mini-batch 200, loss: 34.99566459655762\n",
      "After epoch 7, mini-batch 220, loss: 34.47136461734772\n",
      "After epoch 7, mini-batch 240, loss: 34.544397830963135\n",
      "After epoch 7, mini-batch 260, loss: 34.64103412628174\n",
      "After epoch 7, mini-batch 280, loss: 34.1536602973938\n",
      "After epoch 7, mini-batch 300, loss: 35.11222279071808\n",
      "After epoch 7, mini-batch 320, loss: 34.80895256996155\n",
      "After epoch 7, mini-batch 340, loss: 34.6188006401062\n",
      "After epoch 7, mini-batch 360, loss: 34.97330343723297\n",
      "After epoch 7, mini-batch 380, loss: 34.74741840362549\n",
      "After epoch 7, mini-batch 400, loss: 34.28767395019531\n",
      "After epoch 7, mini-batch 420, loss: 34.77477955818176\n",
      "After epoch 7, mini-batch 440, loss: 34.329386949539185\n",
      "After epoch 7, mini-batch 460, loss: 34.72970688343048\n",
      "After epoch 7, mini-batch 480, loss: 34.493537068367004\n",
      "After epoch 7, mini-batch 500, loss: 34.671762585639954\n",
      "After epoch 7, mini-batch 520, loss: 34.29164516925812\n",
      "After epoch 7, mini-batch 540, loss: 34.52058792114258\n",
      "After epoch 7, mini-batch 560, loss: 34.778316617012024\n",
      "After epoch 7, mini-batch 580, loss: 34.176536321640015\n",
      "After epoch 7, mini-batch 600, loss: 34.67949986457825\n",
      "After epoch 8, mini-batch 20, loss: 34.48994016647339\n",
      "After epoch 8, mini-batch 40, loss: 34.65407609939575\n",
      "After epoch 8, mini-batch 60, loss: 34.10646104812622\n",
      "After epoch 8, mini-batch 80, loss: 34.21774196624756\n",
      "After epoch 8, mini-batch 100, loss: 35.2270188331604\n",
      "After epoch 8, mini-batch 120, loss: 34.922906279563904\n",
      "After epoch 8, mini-batch 140, loss: 34.93284785747528\n",
      "After epoch 8, mini-batch 160, loss: 34.91544842720032\n",
      "After epoch 8, mini-batch 180, loss: 33.94454753398895\n",
      "After epoch 8, mini-batch 200, loss: 34.46837091445923\n",
      "After epoch 8, mini-batch 220, loss: 34.6906795501709\n",
      "After epoch 8, mini-batch 240, loss: 34.39448404312134\n",
      "After epoch 8, mini-batch 260, loss: 33.79965150356293\n",
      "After epoch 8, mini-batch 280, loss: 33.91017460823059\n",
      "After epoch 8, mini-batch 300, loss: 34.27712941169739\n",
      "After epoch 8, mini-batch 320, loss: 34.647104024887085\n",
      "After epoch 8, mini-batch 340, loss: 34.17134988307953\n",
      "After epoch 8, mini-batch 360, loss: 34.290873646736145\n",
      "After epoch 8, mini-batch 380, loss: 34.15571439266205\n",
      "After epoch 8, mini-batch 400, loss: 34.45117509365082\n",
      "After epoch 8, mini-batch 420, loss: 34.38533818721771\n",
      "After epoch 8, mini-batch 440, loss: 34.04415690898895\n",
      "After epoch 8, mini-batch 460, loss: 34.956127285957336\n",
      "After epoch 8, mini-batch 480, loss: 34.058364391326904\n",
      "After epoch 8, mini-batch 500, loss: 34.523261189460754\n",
      "After epoch 8, mini-batch 520, loss: 34.168508410453796\n",
      "After epoch 8, mini-batch 540, loss: 34.6779670715332\n",
      "After epoch 8, mini-batch 560, loss: 35.120991826057434\n",
      "After epoch 8, mini-batch 580, loss: 33.880186557769775\n",
      "After epoch 8, mini-batch 600, loss: 34.53279411792755\n",
      "After epoch 9, mini-batch 20, loss: 34.33830189704895\n",
      "After epoch 9, mini-batch 40, loss: 33.57242810726166\n",
      "After epoch 9, mini-batch 60, loss: 34.07902801036835\n",
      "After epoch 9, mini-batch 80, loss: 34.21594750881195\n",
      "After epoch 9, mini-batch 100, loss: 34.549081444740295\n",
      "After epoch 9, mini-batch 120, loss: 34.58043575286865\n",
      "After epoch 9, mini-batch 140, loss: 34.07291066646576\n",
      "After epoch 9, mini-batch 160, loss: 34.381009101867676\n",
      "After epoch 9, mini-batch 180, loss: 34.29278814792633\n",
      "After epoch 9, mini-batch 200, loss: 34.265705943107605\n",
      "After epoch 9, mini-batch 220, loss: 34.85083198547363\n",
      "After epoch 9, mini-batch 240, loss: 34.001773834228516\n",
      "After epoch 9, mini-batch 260, loss: 34.81445825099945\n",
      "After epoch 9, mini-batch 280, loss: 34.98142409324646\n",
      "After epoch 9, mini-batch 300, loss: 33.88313841819763\n",
      "After epoch 9, mini-batch 320, loss: 33.88830125331879\n",
      "After epoch 9, mini-batch 340, loss: 34.36682116985321\n",
      "After epoch 9, mini-batch 360, loss: 33.815839529037476\n",
      "After epoch 9, mini-batch 380, loss: 34.013017892837524\n",
      "After epoch 9, mini-batch 400, loss: 34.804314613342285\n",
      "After epoch 9, mini-batch 420, loss: 34.15520215034485\n",
      "After epoch 9, mini-batch 440, loss: 34.17830955982208\n",
      "After epoch 9, mini-batch 460, loss: 34.23402225971222\n",
      "After epoch 9, mini-batch 480, loss: 34.380972385406494\n",
      "After epoch 9, mini-batch 500, loss: 34.20741844177246\n",
      "After epoch 9, mini-batch 520, loss: 34.227378249168396\n",
      "After epoch 9, mini-batch 540, loss: 34.04589128494263\n",
      "After epoch 9, mini-batch 560, loss: 34.65641438961029\n",
      "After epoch 9, mini-batch 580, loss: 34.18730306625366\n",
      "After epoch 9, mini-batch 600, loss: 33.65108323097229\n",
      "After epoch 10, mini-batch 20, loss: 34.035759925842285\n",
      "After epoch 10, mini-batch 40, loss: 34.04664218425751\n",
      "After epoch 10, mini-batch 60, loss: 34.13451278209686\n",
      "After epoch 10, mini-batch 80, loss: 34.23720192909241\n",
      "After epoch 10, mini-batch 100, loss: 34.18144392967224\n",
      "After epoch 10, mini-batch 120, loss: 33.5293927192688\n",
      "After epoch 10, mini-batch 140, loss: 33.96358013153076\n",
      "After epoch 10, mini-batch 160, loss: 34.12917912006378\n",
      "After epoch 10, mini-batch 180, loss: 34.08504569530487\n",
      "After epoch 10, mini-batch 200, loss: 34.0678505897522\n",
      "After epoch 10, mini-batch 220, loss: 34.91312539577484\n",
      "After epoch 10, mini-batch 240, loss: 34.4546902179718\n",
      "After epoch 10, mini-batch 260, loss: 34.35270917415619\n",
      "After epoch 10, mini-batch 280, loss: 34.284679532051086\n",
      "After epoch 10, mini-batch 300, loss: 34.23955225944519\n",
      "After epoch 10, mini-batch 320, loss: 34.15347194671631\n",
      "After epoch 10, mini-batch 340, loss: 34.08740425109863\n",
      "After epoch 10, mini-batch 360, loss: 34.39823794364929\n",
      "After epoch 10, mini-batch 380, loss: 34.108492493629456\n",
      "After epoch 10, mini-batch 400, loss: 33.76313877105713\n",
      "After epoch 10, mini-batch 420, loss: 34.18101608753204\n",
      "After epoch 10, mini-batch 440, loss: 34.42068302631378\n",
      "After epoch 10, mini-batch 460, loss: 34.0849586725235\n",
      "After epoch 10, mini-batch 480, loss: 34.294896364212036\n",
      "After epoch 10, mini-batch 500, loss: 33.76131081581116\n",
      "After epoch 10, mini-batch 520, loss: 33.82552993297577\n",
      "After epoch 10, mini-batch 540, loss: 34.27838957309723\n",
      "After epoch 10, mini-batch 560, loss: 33.872966289520264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 10, mini-batch 580, loss: 33.56989228725433\n",
      "After epoch 10, mini-batch 600, loss: 33.86881935596466\n"
     ]
    }
   ],
   "source": [
    "total_loss = 0\n",
    "for epoch in range(10):\n",
    "    for i, data in enumerate(train_data_loader, 0):\n",
    "        images, classes = data\n",
    "        images, classes = images.to(device).float(), classes.to(device).long()\n",
    "        #images, classes = images.float(), classes.long()\n",
    "        optimizer.zero_grad()\n",
    "        output = net(images)\n",
    "        #print(output)\n",
    "        \n",
    "        loss = loss_function(output, classes)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() \n",
    "        if(i % 20 == 19):\n",
    "            print(\"After epoch {}, mini-batch {}, loss: {}\".format(epoch + 1, i + 1, total_loss))\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            total_loss = 0\n",
    "    total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = FaceDataset('/home/cse/dual/cs5180404/col774/public_test.csv')\n",
    "batch_size = 32\n",
    "test_data_loader = DataLoader(train_data, batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3162514714552039\n",
      "[[ 145    0   92  301  162   45  147]\n",
      " [  11    0   10   38    8    8   23]\n",
      " [  73    0  153  255  184   96  158]\n",
      " [  38    0   80 1145  171   50  137]\n",
      " [  99    0   89  283  345   47  234]\n",
      " [  25    0   61  135   56  359   81]\n",
      " [  67    0   78  291  157   58  463]]\n"
     ]
    }
   ],
   "source": [
    "actual, prediction = [], []\n",
    "for i, data in enumerate(test_data_loader, 0):\n",
    "    images, classes = data\n",
    "    #images, classes = images.to(device).float(), classes.to(device).long()\n",
    "    images, classes = images.float(), classes.long()\n",
    "    actual.extend(classes.squeeze().tolist())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    output = net(images)\n",
    "    \n",
    "    prediction.extend(torch.argmax(output, dim=1).squeeze().tolist())\n",
    "    #print(output)\n",
    "print(f1_score(actual, prediction, average='macro'))\n",
    "print(confusion_matrix(actual, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 892   98  919 1621 1097  717 1114]\n",
      "[ 458    0  563 2448 1083  663 1243]\n"
     ]
    }
   ],
   "source": [
    "print(np.bincount(np.array(actual)))\n",
    "print(np.bincount(np.array(prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
